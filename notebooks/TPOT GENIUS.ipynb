{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "from tpot import TPOTRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('small_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"death\", \"severity\", \"days_at_hosp\"], axis=1)\n",
    "death = data.death.astype(int)\n",
    "severity = data.severity.astype(int)\n",
    "days = data.days_at_hosp.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_train_test(y_train, y_test, name):\n",
    "    print(name)\n",
    "    print('train')\n",
    "    sns.histplot(y_train)\n",
    "    plt.show()\n",
    "    print('test')\n",
    "    sns.histplot(y_test)\n",
    "    plt.show()\n",
    "    print('=======')\n",
    "    \n",
    "def draw_hist(y, name):\n",
    "    print(name)\n",
    "    sns.histplot(y)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "Xd_train, Xd_test, yd_train, yd_test = train_test_split(\n",
    "    X, death, train_size=0.75, test_size=0.25, random_state=47\n",
    ")\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(\n",
    "    X, severity, train_size=0.75, test_size=0.25, random_state=47\n",
    ")\n",
    "Xday_train, Xday_test, yday_train, yday_test = train_test_split(\n",
    "    X, days, train_size=0.75, test_size=0.25, random_state=47\n",
    ")\n",
    "\n",
    "draw_train_test(yd_train, yd_test, 'death')\n",
    "draw_train_test(ys_train, ys_test, 'severity')\n",
    "draw_train_test(yday_train, yday_test, 'day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sampling(X, y):\n",
    "    # Class count\n",
    "    counts = y.value_counts()\n",
    "\n",
    "    # Divide by class\n",
    "    data_classes = []\n",
    "    for c in counts.keys():\n",
    "        data_classes.append(X[y==c])\n",
    "\n",
    "    max_count = np.max(counts)\n",
    "    new_datasets = []\n",
    "    new_ys = []\n",
    "    for i, c, k in zip(range(0,len(counts)),counts, counts.keys()):\n",
    "        if c<max_count:\n",
    "            new_datasets.append(data_classes[i].sample(max_count, replace=True))\n",
    "        else:\n",
    "            new_datasets.append(data_classes[i])\n",
    "        new_ys.append(np.full(max_count, k))\n",
    "    new_X = pd.concat(new_datasets, axis = 0)\n",
    "    new_y = np.array(new_ys).flatten()\n",
    "    return new_X, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd_train, yd_train = over_sampling(Xd_train, yd_train)\n",
    "Xs_train, ys_train = over_sampling(Xs_train, ys_train)\n",
    "# Xday_train, yday_train = over_sampling(Xday_train, yday_train)\n",
    "\n",
    "draw_hist(yd_train, 'death')\n",
    "draw_hist(ys_train, 'severity')\n",
    "draw_hist(yday_train, 'days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_classification(y_test, y_pred):\n",
    "    CM = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print('Overall stat:')\n",
    "    print(f'Accuracy: {np.round(accuracy_score(y_test, y_pred), 3)}')\n",
    "    print(f'Precision: {np.round(precision_score(y_test, y_pred, average=\"micro\"), 3)}')\n",
    "    print(f'Recall: {np.round(recall_score(y_test, y_pred, average=\"micro\"), 3)}')\n",
    "    print(f'F1: {np.round(f1_score(y_test, y_pred, average=\"micro\"), 3)}')\n",
    "\n",
    "    print('\\nBy classes stat:')\n",
    "    scores = {\n",
    "        'Accuracy': np.round(CM.diagonal()/CM.sum(axis=1), 3),\n",
    "        'Precision': np.round(precision_score(y_test, y_pred, average=None), 3),\n",
    "        'Recall': np.round(recall_score(y_test, y_pred, average=None), 3),\n",
    "        'F1': np.round(f1_score(y_test, y_pred, average=None), 3)\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_death = TPOTClassifier(\n",
    "    generations=100,\n",
    "    mutation_rate=0.9,\n",
    "    crossover_rate=0.1,\n",
    "    population_size=10,\n",
    "    verbosity=2,\n",
    "    n_jobs=10,\n",
    ")\n",
    "tpot_death.fit(Xd_train, yd_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yd_pred = tpot_death.predict(Xd_test)\n",
    "check_classification(yd_test, yd_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_sev = TPOTClassifier(\n",
    "    generations=100,\n",
    "    population_size=20,\n",
    "    verbosity=2,\n",
    "    n_jobs=10,\n",
    ")\n",
    "tpot_sev.fit(Xs_train, ys_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ys_pred = tpot_sev.predict(Xs_test)\n",
    "check_classification(ys_test, ys_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def err_score(y_test, y_pred):\n",
    "    return np.sum(np.abs(y_pred-y_test))/np.sum(y_test)\n",
    "\n",
    "def scores(y_test, y_pred):\n",
    "    err = np.round(err_score(y_test, y_pred), 3)\n",
    "    R2 = np.round(r2_score(y_test, y_pred), 3)\n",
    "    print(f'err = {err}')\n",
    "    print(f'R2 = {R2}')\n",
    "    return err, R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_alive = data[data.death==0]\n",
    "X_alive = data_alive.drop([\"death\", \"severity\", \"days_at_hosp\"], axis=1)\n",
    "days_alive = data_alive.days_at_hosp.astype(int)\n",
    "\n",
    "data_dead = data[data.death==1]\n",
    "X_dead = data_dead.drop([\"death\", \"severity\", \"days_at_hosp\"], axis=1)\n",
    "days_dead = data_dead.days_at_hosp.astype(int)\n",
    "\n",
    "Xalive_train, Xalive_test, yalive_train, yalive_test = train_test_split(\n",
    "    X_alive, days_alive, train_size=0.75, test_size=0.25, random_state=47\n",
    ")\n",
    "\n",
    "Xdead_train, Xdead_test, ydead_train, ydead_test = train_test_split(\n",
    "    X_dead, days_dead, train_size=0.75, test_size=0.25, random_state=47\n",
    ")\n",
    "\n",
    "draw_train_test(yalive_train, yalive_test, 'alive')\n",
    "draw_train_test(ydead_train, ydead_test, 'dead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_dead = TPOTRegressor(\n",
    "    generations=100,\n",
    "    population_size=20,\n",
    "    verbosity=2,\n",
    "    n_jobs=10,\n",
    ")\n",
    "tpot_dead.fit(Xdead_train, ydead_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydead_pred = tpot_dead.predict(Xdead_test)\n",
    "scores(ydead_test, ydead_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_alive = TPOTRegressor(\n",
    "    generations=100,\n",
    "    population_size=20,\n",
    "    verbosity=2,\n",
    "    n_jobs=10,\n",
    ")\n",
    "tpot_alive.fit(Xalive_train, yalive_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yalive_pred = tpot_alive.predict(Xalive_test)\n",
    "scores(yalive_test, yalive_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5).fit(X)\n",
    "#     return pd.DataFrame(\n",
    "#         imputer.fit_transform(data), columns=data.columns, index=data.index\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    imputer,\n",
    "    tpot_death.fitted_pipeline_,\n",
    "    tpot_sev.fitted_pipeline_,\n",
    "    tpot_dead.fitted_pipeline_,\n",
    "    tpot_alive.fitted_pipeline_,\n",
    "]\n",
    "names = [\n",
    "    \"imputer\",\n",
    "    \"tpot_death\",\n",
    "    \"tpot_sev\",\n",
    "    \"tpot_dead\",\n",
    "    \"tpot_alive\",\n",
    "]\n",
    "\n",
    "for m, n in zip(models, names):\n",
    "    print(n)\n",
    "    with open(f'models/{n}.pickle', 'wb') as f:\n",
    "        pickle.dump(m, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOME MODEL TESTIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Death Prediction\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier\n",
    "estimators = [\n",
    "('lr', LogisticRegression()),\n",
    "('svc', SVC(C=0.001,degree=1,coef0=1)),\n",
    "('gbc',GradientBoostingClassifier(n_estimators=17,max_depth=2))\n",
    "]\n",
    "clf_death = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression())\n",
    "c=StandardScaler().fit(Xd_train)\n",
    "clf_death.fit(c.transform(Xd_train), yd_train)\n",
    "\n",
    "yd_pred = clf_death.predict(c.transform(Xd_test))\n",
    "check_classification(yd_test, yd_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Days of illness prediction\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression,SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "cl=MinMaxScaler().fit(Xalive_train)\n",
    "clf_alive = GradientBoostingRegressor()\n",
    "clf_alive.fit(cl.transform(Xalive_train), yalive_train)\n",
    "\n",
    "yalive_pred = clf_alive.predict(cl.transform(Xalive_test))\n",
    "scores(yalive_test, yalive_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
